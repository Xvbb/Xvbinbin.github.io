<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>随笔</title>
      <link href="/2021/12/14/sui-bi/"/>
      <url>/2021/12/14/sui-bi/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 12 Natural Language Generation</title>
      <link href="/2021/12/14/cs224n-lecture-12-natural-language-generation/"/>
      <url>/2021/12/14/cs224n-lecture-12-natural-language-generation/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> cs224n </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 11 - Question Answering</title>
      <link href="/2021/12/12/cs224n-lecture-11-question-answering/"/>
      <url>/2021/12/12/cs224n-lecture-11-question-answering/</url>
      
        <content type="html"><![CDATA[<p>QA系统的模型有两种，一种是基于LSTM的attention模型，一种是在bert模型基础上微调。</p><h1 id="BiDAFF：the-Bidirectional-Attention-Flow-model"><a href="#BiDAFF：the-Bidirectional-Attention-Flow-model" class="headerlink" title="BiDAFF：the Bidirectional Attention Flow model"></a>BiDAFF：the Bidirectional Attention Flow model</h1><hr><p>整体结构从下往上：</p><ul><li>字符嵌入层</li><li>词嵌入层</li><li>短语嵌入层</li><li>Attention Flow层</li><li>modeling层</li><li>输出层<br><img src="1.png" alt></li></ul><h2 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h2><hr><p>编码部分包括了字符嵌入、词嵌入、短语嵌入层。<br>对文本和问题，分别作编码工作。字符嵌入通过CNN和max pooling实现，词嵌入使用GloVe实现串联，然后各自使用2个Bi-LSTM产生文本和问题的上下文关联嵌入向量。<br><img src="2.png" alt></p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><hr><p>Attention Flow层用于获取文本和问题之间的关联，在BiDFA论文中有两种表示方式：</p><ul><li>从文本到问题的注意力：对文本的每个词，匹配问题中最相关的词</li><li>从问题到文本的注意力：对于问题中的一些词，在文本中选择和（每一个）他们最相关的词</li></ul><p>实现方式如下：<br>第一步，$c_i$是文本中每一对上下文向量，$q_j$是问题中的每一对上下文向量，把$c_i$、$q_j$、$c_i$和$q_j$的逐个元素相乘的乘积得到的乘积向量（element-wise multiplication）链接起来，和之前学习得到的$w_{sim}^{T}，w_sim\in \mathbb{R}^{6H}$向量做内积，得到相似度分数$S_{i,j}$,一个数或者标量。<br>第二步，计算从文本到问题的注意力。$S_{i,j}$矩阵的每一行对应一个文本单词，对每一行做softmax，得到$\alpha_{i,j}$，然后和每个问题向量$q_j$相乘求和，得到$a_i$。<br>第三步，计算从问题到文本的注意力。对$S_{i,j}$每一行取最大值，表示和问题最相关的单词，然后做softmax得到$\beta_i$，和文本向量$c_i$相乘累加得到$b_i$。<br>最后将$c_i$,$a_i$,$c_i\, a_i$的每个元素内积的和，$c_i\, b_i$的每个元素内积的和链接起来，得到最终的输出，代表文本里的某个词。<br><img src="3.png" alt></p><h2 id="模型和输出层"><a href="#模型和输出层" class="headerlink" title="模型和输出层"></a>模型和输出层</h2><hr><p>模型层把Attention层的输出$g_i$传递给两个Bi-LSTM层。</p><ul><li>注意力层对问题和文本的词之间的相互影响建模</li><li>模型层对文本内的词的相互影响建模</li></ul><p>$$m_i = BiLSTM(g_i)\in \mathbb(R)^{2H}$$</p><p>输出层实质是两个分类器，分别预测答案在文本中过的起点和终点。<br>连接$g_i$和$m_i$，和$w_{start}^{T}$求内积，可以得到文本中每个位置的分数，然后softmax得到答案起点概率。类似，另一个分类器先对$m_i$做BiLSTM，然后和$g_i$连接起来，和$w_{end}^{T}$求内积，得到答案终点的概率。计算公式如下：</p><p>$$p_{start} = softmax(w_{start}^{T}[g_i;m_i])$$</p><p>$$m_i^\prime = BiLSTM(m_i)\in \mathbb{R}^{2H}\,\\p_{end} = softmax(w_{end}^{T}[g_i;m_i^\prime ])\,\,\,\,\,\,w_{start},w_{end}\in \mathbb{R}^{10H}$$</p><p>$$\iota(loss)= -log\,p_{start}(s^{*}) - log\,p_{end}(e^{e})$$</p><p><img src="4.png" alt></p><h1 id="BERT模型在阅读理解的应用"><a href="#BERT模型在阅读理解的应用" class="headerlink" title="BERT模型在阅读理解的应用"></a>BERT模型在阅读理解的应用</h1><hr><p>BERT模型是深度双向Transformer在大文本上的编码预训练，主要训练掩饰语言模型masked language model(MLM)和对下个句子的预测next sentence prediction(NSP)。BERT-base有12层1.1亿个参数，BERT-large有24层3.3亿个参数。<br>BERT模型把问题作为片段A，文本片段B，拼接起来作为输入，进行句子嵌入，答案是在片段B里对两个端点的预测，用两个softmax得到，损失函数和MiDAF一样。<br><img src="5.png" alt></p><h2 id="BERT和BiDAF的比较"><a href="#BERT和BiDAF的比较" class="headerlink" title="BERT和BiDAF的比较"></a>BERT和BiDAF的比较</h2><hr><p>两者的区别在于：</p><ul><li>BERT模型具有比BiDAF更多的参数。</li><li>BERT建立于Transformers——没有循环结构并且易于并行，BiDAF建立于数个BiLSTM。</li><li>BERT是预训练模型，BIDAF建立于GloVe——所有参数都需要从有监督数据集学习。</li></ul><p>两者的相似点在于：</p><ul><li>模型都旨在建立问题和文本之间的相互影响关系</li><li>BERT模型使用了问题和文本的连接的自我注意力。<br>attention(Paragraph,P) + attention(P,Question) + attention(Q,P) + attention(Q,Q)</li><li>研究表明，对BIDAF添加文本注意力attention(P,P)构建自我注意力层，性能会得到提升</li></ul><h2 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h2><hr><p>SpanBERT模型修改了BERT的mask。BERT是随机选取15%的词mask，SpanBERT选择连续词块，使用两个词块的端点去预测所有掩饰掉的词——把所有的块内信息集中在端点上。<br>$$y_i = f(x_{s-1}),x_{e+1},p_{i-s+1}$$</p><p><img src="6.png" alt></p><p>但是机器阅读理解的依然没有解决。现在的模型往往只能对一类语料库测试表现良好，泛化能力欠佳。哪怕是BERT-large也不能处理一些简单的问题。</p><h1 id="开放领域问答"><a href="#开放领域问答" class="headerlink" title="开放领域问答"></a>开放领域问答</h1><hr><p>不同于机器阅读理解，开放领域问答假设拥有大量文档（如Wikipedia），但是不知道答案在什么位置，任务的目标是返回任意开放问题的答案。对之建立retrieve-reader框架。<br>在DRQA里，</p><ul><li>输入：大量文档，问题</li><li>输出：代表答案的字符串</li><li>retriever：检索得到和问题最相关的K个文档，K是预先定义的小数值，例如100。是一个标准的TF-IDF信息检索模型。</li><li>reder：在K个文档里找出答案A。是一个学习得到的自然阅读理解模型，在SQuAD和其他远距离监督QA数据集上训练得到。</li></ul><p>可以对retriver进行训练。对retriver和reader联合训练，每一个文本可以用BERT编码为向量，问题分布和文章分布的内积可以作为信息检索分数。但是这在有大量文章的情况下不容易建模。<br><img src="7.png" alt></p><p>稠密文章检索Dense passage retrieval(DPR)，使用问题-答案对，对检索器进行训练，可以获得比常规信息检索模型更好的表现。</p><p>近期的工作表明，用生成答案的方式取代提取答案的方式，是有益的。</p><p>大模型，例如T5，不使用明确的检索器，可以在开放领域问答取得很好的效果，<br><img src="8.png" alt></p><p>也有不使用reader，把所有短语用密集向量编码，在推理时候脱离BERT模型，只做近邻搜索的方式。<br><img src="9.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> cs224n </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention Is All Your Need</title>
      <link href="/2021/12/11/attention-is-all-your-need/"/>
      <url>/2021/12/11/attention-is-all-your-need/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/abs/1706.03762v5" target="_blank" rel="noopener">Attention Is All Your Need</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>在2017年，LSTM在句子模型和翻译问题中是最先进的技术，常规RNN将位置与计算时间中的步骤对齐，它们生成一个隐藏状态序列ht，作为前一个隐藏状态ht-1和位置t的输入的函数。这种线性顺序的计算不利于并行化。Attention机制允许建立依赖关系，不考虑计算过程中输入和输出序列的距离问题。但绝大部分时候和RNN结合在一起使用。Transformer模型是首个完全依赖attention机制实现输入和输出序列全局依赖的模型，具有良好并行性。</p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><hr><p>减少顺序计算的目标，是拓展神经GPU、位图和ConvS2S的基础。所有这些都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。在这些模型里,两个来自任意输入或者输出位置的相关信号需要的操作数，随着距离间隔增长，在ConvS2S和ByteNet是线性增长。这使得学习两个遥远位置的依赖变得困难。在Transformer中操作数被减少到常数级，尽管以通过平均化attention权重位置减少有效分辨率（resolution）为代价，本文将之描述为多头注意力。</p><p>自我注意力（Self-attention），有时候也被称为内部注意力，是一种将单个序列的不同位置联系起来以计算序列表示的机制。自我注意力在很多领域被广泛使用，例如阅读理解、摘要总结、文本蕴涵和学习任务无关的句子表示。</p><p>端到端记忆网络，基于重复注意机制而不是序列排列的重复，已被证明在简单语言问题回答和语言建模任务中表现良好。</p><p>Transformer是首个基于自我注意力计算输入输出的分布表达，而不是序列对齐RNN或者卷积的转换模型。</p><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><hr><p>Encdoer把输入序列$(x_1,\ldots,x_n)$和连续分布$z=(z_1,\ldots,z_n)$连接起来。给定$z$，decoder会生成符号序列$(y_1,\ldots, y_m)$，每次一个元素。在每一个时刻，模型都是自回归的，在生成下一个符号时，把之前生成的符号作为额外信息附加到输出上。<br>Transformer模型整体架构使用堆叠的自注意力和点方向，把encoder和decoder层完全连接起来。<br><img src="model%20architecture.png" alt="Transformer整体架构"></p><h2 id="Encoder和Decoder的堆栈"><a href="#Encoder和Decoder的堆栈" class="headerlink" title="Encoder和Decoder的堆栈"></a>Encoder和Decoder的堆栈</h2><hr><p>Encoder：encoder由6个独立层构成，每一层有两个自称。第一层是多头自我注意力机制，第二层是简单的位置智能的全连接前馈网络。在每个子层使用残差连接，做层标准化处理。就是说，每一子层的输出是$LayerNorm(x+Sublayer(x))$，$SubLayer(x)$是由子层实现的函数。所有的子层，和embadding层一样，产生的输出维度是512。</p><p>Decoder：decoder也是6个独立层构成。在两个子层的基础上，增加了一个子层，表示对encoder输出的多头注意力。和encoder层一样，每个子层用残差连接，并且做标准化处理。同时对decoder层中的自我注意力子层做了修改，以防止在对当前位置计算时，把后续位置的信息也纳入计算。通过masking，并且把输出embedding偏移一个位置，确保了对位置i的预测只能依赖于位置小于i的已知输出。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><hr><p>Attention函数可以被描述为把query和一系列key-value的映射作为output，query、keys、values和output都是向量。Output是values的权重和，分配给每个value的权重由query的兼容函数与相应的key计算。</p><h3 id="Scaled-Dot-product-Attention"><a href="#Scaled-Dot-product-Attention" class="headerlink" title="Scaled Dot-product Attention"></a>Scaled Dot-product Attention</h3><hr><p>把特定的attention称为“Scaled Dot-Product Attention”。输入由$d_k$维的query和key、$d_v$维的value组成。计算query和所有的key的内积，每一项除以$\sqrt{d_k}$，通过softmax函数获得value的权值。</p><p>把一系列query的Attention函数的输出用矩阵Q表示，key和value用矩阵K和V表示，矩阵的输出可以表示为：<br>$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})\tag{1}$$</p><p>使用最广泛的attention函数是additive attention和dot-product attention。除了计算缩放因子$(\frac{1}{\sqrt{d_k}})，$Dot-product attention在整个算法中是一致的。Additive attention通过前馈网络计算单个隐藏层的compatibility函数。这两个函数理论上复杂度相近，但实际上dot-product attention更快并且有更好的空间效率，因此可以用高度优化的矩阵乘法代码实现。</p><p>对于较小的$d_k$这两种方式表现相近，在不实用缩放因子$d_k$时additive attention表现得更好。作者认为对于较大值的$d_k$，内积变得非常巨大，使对应softmax函数区域的梯度变得非常小。为了抵消这方面的影响，把内积缩小为$\frac{1}{\sqrt{d_k}}$。</p><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><hr><p>相比较使用$d_{model}$-demensional的query、key、value的单一attention函数，用不同的学习得到的线性投影，把key、value、query分别线性映射h次到$d_k$，$d_k$，和$d_v$维上更加有益。在每一次显现投影，并行调用attention函数，生成$d_v$-dimensional的输出值。把它们连接起来并再次映射，最后得到最终的value。多注意力头允许模型把来自不同分布子空间不同位置的信息结合起来。通过单个注意力头，平均化inhibit。<br>$$MultiHead(Q,K,V) = Concat(head_1,\ldots,head_h)W^O\\where\,head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$<br>映射矩阵$W_i^Q\in \mathbb{R}^{d_{model}\times d_k}$，$W_i^K\in \mathbb{R}^{d_{model}\times d_k}$，$W_i^V\in \mathbb{R}^{d_{model}\times d_v}$，$W^O\in \mathbb{R}^{hd_{v}\times d_model}$。<br>设置h=8层并行的attention层或者注意力头，$d_k=d_v=\frac{d_model}{h}=64$。优于注意力头的降维效果，整个计算的开销和不降维的单注意力头attention相似。<br><img src="Figure2.png" alt></p><h3 id="模型中attention的应用"><a href="#模型中attention的应用" class="headerlink" title="模型中attention的应用"></a>模型中attention的应用</h3><hr><p>Transformer在三方面使用多注意力头：</p><ul><li>在“encoder-decoder”层，query来自前一decoder层，存储的key和value来自前一encoder层。使得decoder的每一个位置都和整个输入序列关联起来。这个实现方式模仿了Seq2Seq模型中经典的encoder-decoder attention机制。</li><li>encoder包含了自我注意力层。在自我注意力层汇总所有的key、value、query都来自前一encoder层的output。Encoder的每一个位置都和前一encoder层关联起来。</li><li>decoder的自我注意力层允许decoder的每一个位置，和decoder中的其他所有位置，包括自身位置关联起来。需要防止信息的向左流动，以保持自回归特性。在scaled dot-product attention中，通过对softmax的非法链接的所有value做mask——把value设置成-∞实现。</li></ul><h2 id="position-wise-前馈网络"><a href="#position-wise-前馈网络" class="headerlink" title="position-wise 前馈网络"></a>position-wise 前馈网络</h2><hr><p>每一个attention子层，包含一个全连接前馈网络，独立且相同地应用在每个位置上。前馈网络使用RelU激活并包含两个线性转换。<br>$$FFN(x)=max(0,xW_1+b_1)W_2+b_2\tag{2}$$<br>对于不同位置，线性转换的方式是相同的，但不同层使用不同的参数值。另一种方式描述是两个卷积，内核大小是1。输入和输入的维度$d_{model}=512$，inner-layer的维度$d_{ff}=2048$</p><h2 id="Embedding和Softmax"><a href="#Embedding和Softmax" class="headerlink" title="Embedding和Softmax"></a>Embedding和Softmax</h2><hr><p>和其他句子转换模型相似，Transformer使用学习得到的embedding把输入的token和输出的token转变为$d_{model}$维度的向量。同时使用常规的学习得到的线性转换和softmax函数，把decoder的输出转变成后续token可能性的预测。两个embedding层之间和pre-softmax线型转变共用相同的权重矩阵。在embedding层，把权重乘以$\sqrt{d_{model}}$。</p><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><hr><p>Transformer不包含循环和卷积，为了让模型利用句子的顺序，需要添加token在句子里相对或者绝对位置的信息。<br>在encoder和decoder堆栈的末端，对输入的embedding添加了位置编码。位置编码有和embedding相同的维度$d_{model}$，所以它们可以相加。位置编码的设置有两种选择，学习或者固定顺序。</p><p>本模型采用不同频度的sin和cos函数：<br>$$PE_(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\ \,\\PE_(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$</p><p>$pos$表示位置，$i$表示维度。位置编码的每一个维度对应一个正弦信号。选择整个函数是因为假设对于任何给定的$$k$$偏置，$$PE_{pos+k}$$都可以被表示为$PE_{pos}$的线性表示，模型可以比较容易地学习到。正弦函数可以让模型推断出比在训练中遇到的序列长度更长的句子。</p><h1 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h1><hr><p>把Transformer和RNN、CNN作比较，论证为什么选择Transformer。RNN和CNN处理 把可变长度的句子符号分布$(x_1,\ldots,x_n)$映射到另一个登场的句子$(z_1,\ldots,z_n)$，和Transformer在一个隐藏层里decoder或者encoder对句子做的操作类似。<br>一共有三点，一是每一层的整体计算复杂度，另一点是可并行计算的单元数量——用所需的最小顺序操作数量度量，第三点是网络中长距离依赖的路径长度——网络中不同的层任意两个输入输出的最大路径长度。</p><p><img src="table1.png" alt></p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><hr><p>四个超参，使用Adam优化器，设置$\beta_1=0.9,\beta_2=0.98,\epsilon=10^{-9}$,根据公式在训练中改变学习率：<br>$$lrate=d_{model}^{-0.5}\cdot min(step_num^{-0.5}, step_num\cdot warmup_steps^{-1.5})\tag{3}$$</p><p>使用了两种类型的正则化方式。</p><ul><li>离散Dropout：在每一个子层的output被加入子层和标准化前，对output做dropout。并且对embedding的和，与encoder和decoder的堆栈里位置编码做dropout。设置$P_{drop}=0.1$。</li><li>标签平滑化：设置平滑值$\epsilon_{ls}=0.1$,虽然会增加困惑度，但提升了准确率和BLEU分数。</li></ul><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><hr><p>Transformer模型是是首个完全依托于attention的句子转换模型，用多头自我注意力代替了广泛用于encoder-decoder结构中的循环层。在机器翻译中取得SOTA。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><blockquote><p>Attention Is All Your Need<br>Layer normalization<br>Neural machine translation by jointly learning to align and translate<br>Long short-term memory-networks for machine reading</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> cs224n </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文研读 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N attention学习笔记</title>
      <link href="/2021/12/08/cs224n-attention-xue-xi-bi-ji/"/>
      <url>/2021/12/08/cs224n-attention-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p><img src="1.png" alt><br>Attention</p><p>• 常规Seq2Seq的RNN网络，存在imformation bottleneck问题.如图所示，所有源语句的信息都存储在encoder的一个隐藏状态里，所有的生成信息都基于此形成.attention的设计可以解决这问题.<br>• 核心思想是，在decoder的每一时序，通过decoder和encoder的direct link，使注意力集中在一部分词句上，从而判断接下来是什么词.</p><p><img src="2.png" alt><br>encoder照常，每一个时序的decoder，都会调用encoder每一个位置的隐藏状态信息，作内积，得到attention scores.为每一个隐藏状态打分后，通过softmax得到attention分布.从分布可以看出当前decoder应该focus on在哪个隐藏状态上.</p><p><img src="3.png" alt><br>把attention分布里各项相加，得到attention输出，表示encoder的隐藏状态，包含的信息主要是受到高注意力的那些隐藏状态.<br>将输出和decoder的起始状态链接，得到$\hat y$，预测第一个词的输出，如图所示，是he.<br>之后处理后面位置的decoder，类似，与每个位置的encoder元素的隐藏状态作内积作为打分，重复之前步骤</p><p><img src="4.png" alt></p><p>最后得到完整的输出序列</p><p><img src="5.png" alt></p><p>公式如图.同时attention的表示有若干种变体，以针对不同情形进行改进</p><p><img src="6.png" alt></p><p>对于attention，更为广泛的解释是:</p><blockquote><p>Given a set of vector values， and a vector query， attention is a technique to compute a weighted sum of the values， dependent on the query</p></blockquote><p>给定一个query向量和value向量，attention是一种依据query，计算value权重和的技术.<br>query决定了要在哪些value上pay attention，权重和是这些被挑选出来的value包含的<br>选择性求和.<br>attention是一种获得任意表示集(value)的固定大小表示的方法，它依赖于某些其他表示(query).<br>attention最初被用于机器翻译，后来被拓展到多个方向和领域，并促成transformer和bert的发展.</p><p>对于常规的RNN，是从左往右铺开的，对线性布局启发式编码，相近的词会互相影响含义.但是这样的神经网络需要跨越O(n)步长使得距离为n的单词互相作用.这意味着因为梯度问题长距离依赖很难被处理，并且线性顺序嵌入的单词，不利于句子语义的表示.这样的结构缺少并行性.</p><p><img src="7.png" alt></p><p>先不考虑循环，引入word windows模型</p><p><img src="8.png" alt><br>图片的数字表示可以计算状态之前的最小步数.不可并行操作的数量，不会影响句子的长度.</p><p><img src="9.png" alt><br>堆叠单词窗口层允许更多单词之间的交互.<br>最大交互距离=序列长度/窗口大小，但如果序列太长，就会忽略长距离的上下文.</p><p>回到attention上，attention可以把每个词的表示视作一个query，合并来自一系列value表示的信息.前面说的attention是从encoder到decoder，在这attention用在单个句子里.</p><p><img src="10.png" alt><br>每一层都可以和前一层的所有单词交互</p><p>引入self-attention概念<br><img src="11.png" alt><br>每个词都有qkv，从第二层开始，每一层的qkv取值为前一层的输出x.依次计算key-query的关联度，从关联度计算attention权重，把权重和作为value.</p><p><img src="12.png" alt><br>self-attention是在set上的操作，是无序的，输出的顺序是未知的.</p><p><img src="13.png" alt><br>对于第一个问题，把位置信息编码进kqv，把句子序号作为一个向量，和每个qkv相加</p><p><img src="14.png" alt></p><p>对于p的表示有两种思路<br><img src="15.png" alt><br>一是用正弦曲线向量表示位置，优点在于绝对位置不那么重要，正弦曲线有周期性，可以推断更长的序列</p><p>二是从头开始，用绝对位置表示，每个$p_i$表示一列，$p_i$是一个可学习的参数.优点是每个位置数据都可以被学习.</p><p>对于第二个问题，在self-attention中加入线性相关性<br>对没一个输出向量，增加一个前馈神经网络.<br><img src="16.png" alt></p><p>对于第三个问题，把一部分将来会出现的单词遮掩，在每一时序，改变keys和values，使之仅包含过去出现过的词.考虑并行性，设置未出现的词的attention分数为-∞.</p><p><img src="18.png" alt></p><p><img src="19.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> cs224n </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/12/08/hello-world/"/>
      <url>/2021/12/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
