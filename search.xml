<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ERNIE: Enhanced Representation through Knowledge Integration学习笔记</title>
      <link href="/2021/12/17/ernie-enhanced-representation-through-knowledge-integration-xue-xi-bi-ji/"/>
      <url>/2021/12/17/ernie-enhanced-representation-through-knowledge-integration-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/pdf/1904.09223.pdf" target="_blank" rel="noopener">ERNIE: Enhanced Representation through Knowledge Integration</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>ERINE（Enhanced Representation through Knowledge Integration通过知识集成增强表示），通过实体层面的遮掩和短语层面的遮掩增强知识，来学习语言表达。实体级策略遮掩常常由几个单词组成的实体，短语级策略掩盖了由几个词作为一个概念单元组合而成的整个短语。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>大量模型通过上下文来预测缺失的词以学习模型的表达，没有考虑句子的先验知识。例如“ Harry Potter is a series of fantasy novels written by J. K. Rowling”，“Harry Potter”是小说名字，“J. K. Rowling”是作者。在没有长上下文的帮助下，语言模型也很容易通过单词搭配预测缺失的实体“Harry Potter”，但是模型无法根据“Harry Potter”和“J. K. Rowling”的关系来预测“Harry Potter”。所以直觉上如果模型可以学习更多的先验知识，模型可以获得更可靠的语言表达。<br>ERINE使用知识遮掩策略：短语级策略和实体级策略。把由若干词组成的一个短语或者一个实体视作一个单元，在词表达训练过程中，在相同单元里的所有单词都被遮掩，而不是只有单个词或者字符被遮掩。通过这种方式，在训练过程中隐式地学习短语的先验知识和实体。ERINE隐式地学习知识信息和长语义依赖，如实体之间的关系、实体的属性、事件的类型，以指导词嵌入学习。<br>为了减少模型的训练开销，ERINE在多种中文数据上预训练，在5种中文NLP任务中SOTA。</p><p>本文的主要贡献在于：</p><ul><li>介绍新的语言模型学习方式，通过遮掩短语和实体这样的单元，从单元隐式地学习句法和语义信息。</li><li>ERINE在中文自然语言处理任务上取得了比前SOTA更好的成绩</li><li>开源ERINE代码和预训练模型，[源码](<a href="https://github.com/PaddlePaddle/" target="_blank" rel="noopener">https://github.com/PaddlePaddle/</a><br>LARK/tree/develop/ERNIE)</li></ul><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="上下文独立的表达"><a href="#上下文独立的表达" class="headerlink" title="上下文独立的表达"></a>上下文独立的表达</h2><hr><p>曾经流行的一种评估神经网络语言模型的模型结构，是用带有线性映射层和非线性隐藏层的前馈神经网络去学习词向量表达。</p><p>通过大量无标签数据学习广泛的语言表达，预训练一个语言模型是搞笑的。传统方法聚焦与上下文依赖的词嵌入。Word2Vec和Glove采用大语料库作为输入，然后产生上百维的词向量。这类语言模型为词表中的每个词生成单一的词向量嵌入表达。</p><h2 id="上下文敏感的表达"><a href="#上下文敏感的表达" class="headerlink" title="上下文敏感的表达"></a>上下文敏感的表达</h2><hr><p>在文本中一个词可以有完全不同的语义。Skip-thought提出了一种通用分布式句子编码器的无监督学习方法。Cove表明在大量普通NLP任务汇总，相比较只用无监督的词和字符向量，添加上下文向量可以提升模型性能。ULMFit提出了一种可以应用在任意NLP任务的高效迁移学习方法。ELMo从不同维度对传统词嵌入研究进行了综合。GPT通过调节Transformer加强了上下文敏感嵌入。</p><p>BERT通过两种不同的任务预训练语言模型。BERT随机遮掩句子中一定比例的词然后通过预测他们来学习。BERT也学习预测两个句子是否是临近的（拼接的，例如QA的问题和答案）。这是为了对两个句子的关系进行建模，这是传统语言模型没有的。因此BERT取得了多项SOTA。</p><p>一些其他的研究者尝试基于这些模型，添加更多的信息。MT-DNN组合预训练学习和多任务学习以提升多项GLUE任务的性能。GPT-2在预训练过程中添加任务信息，并使模型适应zero-shot任务。XLM在预训练过程中添加语言嵌入，在跨语言任务取得较好成果。</p><h2 id="异构数据"><a href="#异构数据" class="headerlink" title="异构数据"></a>异构数据</h2><hr><p>在无监督异构数据上预训练语义编码器可以提升迁移学习的性能。大量句子编码器采用从Wikipedia、网页新闻、网页问答页面和论坛提取的异构数据。基于响应预测的句子编码器收益于从Reddit对话提取的问题-响应对数据。XLM把并行语料库引入BERT，与掩模语言模型任务联合训练。通过在易购数据上预训练transformer模型，XLM在监督和无监督MT任务和分类任务取得了很大的性能改进。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="Transformer编码器"><a href="#Transformer编码器" class="headerlink" title="Transformer编码器"></a>Transformer编码器</h2><hr><p>ERNIE使用和GPT、BERT、XLM类似的多层Transformer作为基本编码器，通过自我注意力获取句子里每个token的上下文信息，并且生成上下文嵌入的序列。</p><p>对于中文语料，在CJK Unicode范围里的每个字符周边添加空格，冰使用WordPiece token化中文句子。对于给定的token，它的输入表示由累加token表达、片段和位置嵌入组成。每个句子的第一个token是特殊分类嵌入[CLS]。</p><h2 id="知识增强"><a href="#知识增强" class="headerlink" title="知识增强"></a>知识增强</h2><hr><p>使用先验知识强化与训练模型。提出多场景知识遮掩策略来对语言表达增强短语级和实体级知识。</p><p><img src="1.png" alt="句子的不同遮掩层级"></p><h3 id="基础级遮掩"><a href="#基础级遮掩" class="headerlink" title="基础级遮掩"></a>基础级遮掩</h3><hr><p>基础级遮掩把一个句子作为一个基础语言单元序列，对于英语，基础语言单元是单词，对于中文，基础语言单元是汉字。在训练过程中，随机遮掩15%基础语言单元，句子里的其他基础语言单元作为输入，然后训练一个transformer去预测遮掩的单元。基于基础级遮掩，获取基本的词表示。因为是基于基础语义单元的随机遮掩进行训练，高层级的语义只是很难完全建模。</p><h3 id="短语级遮掩"><a href="#短语级遮掩" class="headerlink" title="短语级遮掩"></a>短语级遮掩</h3><hr><p>短语是一组单词或者字符聚合在一起从而被视为概念上的单元。对于英语，使用词法分析和组块工具来获取句子中短语的边界，并使用一些依赖于语言的分词工具来获取其他语言(如汉语)中的词/短语信息。对于短语级遮掩的场景，也是用基础语言单元作为训练输入，随机选择句子里的少量短语，遮掩并预测同个短语里的所有基础单元，从而把短语信息编码进词嵌入。</p><h3 id="实体级遮掩"><a href="#实体级遮掩" class="headerlink" title="实体级遮掩"></a>实体级遮掩</h3><hr><p>命名实体包含人物、地点、组织、生产商等，他们可以被抽象或者有物理存在。在实体级遮掩中，首先分析句子里的命名实体，遮掩并预测所有实体的空缺。经过三个阶段的学习，获得了被丰富的语义信息增强的词表达。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><hr><p>ERNIE使用和BERT-base相同的模型规模，使用12个编码层，768个隐藏单元和12个注意力头。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> cs224n </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文研读 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding论文研读</title>
      <link href="/2021/12/17/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-lun-wen-yan-du/"/>
      <url>/2021/12/17/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-lun-wen-yan-du/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/pdf/1810.04805" target="_blank" rel="noopener">BERT Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><hr><p>本文提出BERT模型。BERT，即Bidirectional Encoder Representations from Transformers，对transformer进行双向编码表示。BERT把所有层的左右上下文连接起来，对未标签的文本做深层双向表示的预训练。BERT模型通过微调，只要添加一个输出层就能处理大量NLP任务，不需要对任务具体架构进行修改。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>预训练语言模型被证明在多类NLP任务中有效，包含句子层面的任务例如自然语言推断和paraphrasing——通过分析句子整体判断他们的关联，token级别的任务例如命名实体识别和问答。<br>存在两种把预训练语言表示应用在下游任务的策略：基于特征，微调。</p><ul><li>基于特征：<br>例如ELMo，使用任务具体架构，把预训练表示作为额外的特征</li><li>微调：<br>例如GPT，使用极少的任务具体元素，在下游任务训练时对所有参数微调。</li></ul><p>这两种方法在预训练阶段有相同的目标函数，都是使用无向语言模型去学习广泛的语言表示。</p><p>本文认为当前技术限制了预训练表示的发挥，特别是对微调方式来说。主要限制表现为标准语言模型是无向的，这限制了预训练时可选用的结构。例如GPT使用从左到右的的结构，每个token只能在Transformer的自我注意力层添加。这样的限制对句子层面的任务来说不是最优的，并且对例如问答这样的token层面的任务做微调时有很大危害，对这样的任务来说文本的双向表示非常重要。</p><p>本文通过BERT改进微调。BERT使用“Masked Language Model（MLM）”来缓解无向的限制。MLM随机遮掩输入的一部分的token，并且依据现在的文本去预测遮掩的内容。MLM融合了左右文本，使得训练深层的双向Transformer称为可能。通过“预测下一个句子”任务链接预训练文本对的表示。文章主要贡献如下：</p><ul><li>证明了对语言表示做双向预训练的重要性。BERT通过MLM实现对深层双向表示的预训练。</li><li>预训练表示减少了许多高度工程化任务对具体架构的需要。</li><li>BERT取得了11项NLP任务的SOTA。</li></ul><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="基于特征的无监督方法"><a href="#基于特征的无监督方法" class="headerlink" title="基于特征的无监督方法"></a>基于特征的无监督方法</h2><hr><p>学习词的大范围可用表示是近十年的热点研究方向，有多种非神经网络方法和神经网络方法。预训练词嵌入显著提升了从头开始的嵌入学习。为了预训练词嵌入，提出了从左到右的语言模型，以及在左右上下文中区分正确和不正确的单词。<br>这些方法被推广到更粗的粒度，如句子嵌入和段落嵌入。为了训练句子表达，之前的研究是对候选的后文内容进行排名，给定一个前文句子的表达，从左往右生存下一个句子，或者降噪自动编码器。<br>ELMo和他的前辈在不同维度做传统词嵌入研究，在从左往右和从右往左模型提取文本敏感特征。每一个token的上下文表示是从左往右和从右往左表示的连结。也有提出从任务重学习上文稳表示，通过LSTM从左边和右边去预测单个单词的。和ELMo相似，这模型基于特征且非深度双向。后有研究表明完形填空可以被用途提升文本生成模型的健壮性。</p><h2 id="无监督微调的方法"><a href="#无监督微调的方法" class="headerlink" title="无监督微调的方法"></a>无监督微调的方法</h2><hr><p>对于基于特诊过的方式，从无标签文本进行词嵌入，是最早的预训练的方式。<br>在近些时候，可以生成上下文token表示的句子或者文本编码器已经从无标签文本训练得到，在微调后用于有监督的下游任务。这些方法的有点在于，较少的参数需要从头开始学习。GPT模型在GLUE上实现了前SOTA。从左往右模型化和自动编码目标被用于预训练这类模型。</p><h2 id="在有监督数据上转化学习"><a href="#在有监督数据上转化学习" class="headerlink" title="在有监督数据上转化学习"></a>在有监督数据上转化学习</h2><hr><p>也有一些工作研究具备大数据集的有监督任务如何进行效率转化，例如自然语言推论和机器翻译。计算机视觉的研究正式了从大规模的预训练模型转化学习的重要性，一种思路是用ImageNet微调模型预训练。</p><h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><hr><p>BERT框架由两部分组成：预训练和微调。预训练期间，模型使用无标签数据在多种不同的预训练任务中训练。<br>对于微调，BERT模型首先用预训练参数初始化，通过下游任务的有标签数据微调所有的元素。每个下有任务都有独立的微调模型，虽然他们用相同的参数初始化。</p><p><img src="1.png" alt="图1"><br>如图所示的QA样例，除了输出层，预训练和微调程序有相同的整体结构。相同的预训练模型元素被用于初始化模型并用于不同的下游任务。在微调中，所有元素被微调。[CLS]是被添加在每个输入样例前的特殊符号，[SEP]是特殊的分离符号，表示中断，例如分离问题和答案。</p><p>BERT模型的显著特点是其用于不同任务的统一架构。预训练架构和最终用于处理下游任务的架构只有微小的差异。</p><p><strong>模型架构</strong></p><p>BERT模型的架构是基于Transformer原始实现的多层双向Transformer编码器，采用tensor2tensor库。<br>把层数（例如Transformer块）记作$L$，隐藏规模记作$H$，自我注意力头记作$A$。设计了两个模型规模，</p><ul><li>$BERT_{BASE}：L=12，H=768,A=12，Total Parameters=110M$</li><li>$BERT_{LARGE}：L=24，H=1024,A=16，Total Parameters=340M$</li></ul><p>BERT-base的规模和GPT一样大，但BERT使用双向自我注意力，GPT使用受限的自我注意力，每个token只能和它左边的文本相联结。</p><p><strong>输入/输出表达</strong></p><p>为了使BERT能处理大量下游任务，输入表示可以是单一的句子，也可以是一个token序列里的句子对（如&lt;问题，答案&gt;）。对于整个工作，“句子”可以是连续文本的任意块，不需要是确切的语言学意义的句子。“序列”指的是输入BERT的句子，可以是单个句子或者两个句子的组合。</p><p>BERT使用了WordPiece嵌入了30000个token的词表。每个序列的第一个token是特殊的类别token[CLS]。这个token对应的最终隐藏状态被用作分类任务的聚合序列表示。句子队聚合在一起形成单一的序列，用两种方式区分。首先用一个特殊token[SEP]分离他们，然后对每个token添加学习得到的嵌入，以表示他们属于句子A还是句子B。之前的图片里，把输入嵌入记为$E$，[CLS]对应的最终隐藏向量记作$C\in \mathbb{R}^H$，$i^{th}$对应的最终隐藏向量记作$T_i\in \mathbb{R}^H$。<br>对于一个给定的token，它的输入向量由对应的token、片段和位置嵌入累加构成。如图所示：<br><img src="2.png" alt></p><h2 id="预训练BERT"><a href="#预训练BERT" class="headerlink" title="预训练BERT"></a>预训练BERT</h2><hr><p>没有适用传统的从左往右或者从右往左模型预训练BERT，是通过两个无监督任务来实现。</p><ul><li>任务一：Masked LM（MLM），掩饰语言模型<br>随机遮掩一部分输入的token，然后预测遮掩的那些token。遮掩的token对应的最终隐藏向量被送到词汇表上的输出softmax。实验后选择在每个每个序列中随机遮掩15%。<br>尽管这样构成了一个双向预训练模型，但是存在一个缺点——制造了预训练和微调的错误匹配。因为[MASK]token在微调中不出现。为了缓和这个矛盾，对于被选择的token采取三种策略：<ol><li>80%的场合用[MASK]代替这个token。</li><li>10%的场合用另一个随机的token代替。</li><li>10%的场合不改变这个token。</li></ol></li></ul><ul><li><p>任务二：Next Sentence Prediction（NSP），对下个句子的预测<br>为了训练模型理解句子间的关系，预训练一个二值化的NSP任务，该任务可以从任意单语语料库中轻易生成。具体来说，当为每个预训练示例选择句子A和B时，50%的时间B是A后面的实际句子(标记为IsNext)， 50%的时间它是语料库中的随机句子(标记为NotNext)。图1左上的C就被用于NSP。</p><ul><li>NSP的形式如下所示：<br>Input = [CLS] the man went to [MASK] store [SEP]<br>he bought a gallon [MASK] milk [SEP]</li></ul><p>Label = IsNext</p><p>Input = [CLS] the man went to [MASK] store [SEP]<br>penguin [MASK] are flight ##less birds [SEP]</p><p>Label = NotNext</p></li></ul><p>  BERT把左右元素转化，用于初始化端任务模型参数。</p><p><strong>预训练数据</strong></p><p>BERT的语料库是BooksCorpus（8亿词）和英语Wikipedia（25亿词）。对于Wikipedia，为了提取长连续句子，只提取文本文章，忽略列表、表格和标题。</p><h1 id="微调BERT"><a href="#微调BERT" class="headerlink" title="微调BERT"></a>微调BERT</h1><hr><p>Transformer中的自我注意机制可以通过交换适当的输入和输出来为许多下游任务建模，不管它们涉及单个文本还是文本对。对于涉及文本对的应用，一个常见的模式是在应用双向交叉注意前独立编码文本对。BERT利用自我注意力机制将这两个阶段统一起来，因为用自我注意力编码连接的文本对有效地包含了两个句子之间的双向交叉注意力。</p><p>对于每个任务，只需将特定于任务的输入和输出插入BERT中，然后端到端对所有参数进行微调。在输入方面，训练前的句子A和句子B类似于(1)意译时的句子对，(2)暗示时的假设-前提对，(3)问题回答时的问题-段落对，(4)文本分类或序列标记时的简并文本对。在输出处，令牌表示被送入输出层，用于执行令牌级别的任务，如序列标记或问题回答，而[CLS]表示被送入输出层，用于分类，如需要或情感分析。</p><p>对比预训练，微调的成本非常低。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文研读 </tag>
            
            <tag> 语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT-A:Fine-tuning BERT with Adapters and Data Augmentation学习笔记</title>
      <link href="/2021/12/17/bert-a-fine-tuning-bert-with-adapters-and-data-augmentation-xue-xi-bi-ji/"/>
      <url>/2021/12/17/bert-a-fine-tuning-bert-with-adapters-and-data-augmentation-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/pdf/1810.04805" target="_blank" rel="noopener">BERT-A:Fine-tuning BERT with Adapters and Data Augmentation</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>在近年NLP任务处理中有两个趋势，一是预训练模型的广泛应用，二是倾向于构建能以相同性能处理多类任务的单一语言模型。<br>对于前者，诸如ULMFiT、ELMo、Transformer、BERT这类预训练上下文模型，使用预训练的表达来编码上下文信息，而不是从头开始训练一切。<br>本文提出用预训练的方式来处理多类任务。GLUE里的任务多是句子层面的，本文探索用在语境QA中使用类似技术的可能性，在语境QA中，理解句子每一部分的细节是很重要的。并且将之拓展到单一任务，其中性能比参数数量更加重要。</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><hr><h2 id="BERT和微调"><a href="#BERT和微调" class="headerlink" title="BERT和微调"></a>BERT和微调</h2><hr><p>BERT使用双向编码器，每个词都和其他词双向链接，在预训练中使用MLM（Masked Language Modeling），对部分词遮掩并让模型预测。也通过对下一个句子的预测，来理解句子之间的关系，从而获得更深层的上下文关系。<br>之前大多数系统解决NLP任务的方式，是把BERT作为一种方法去获取上下文单词嵌入，并在此基础上构建一个二级模型，这会花很多时间。另一种方式是对每一个任务微调BERT的所有参数，这不具备泛化能力，并且受设备存储性能的制约。</p><h2 id="Answer-Pointer"><a href="#Answer-Pointer" class="headerlink" title="Answer Pointer"></a>Answer Pointer</h2><hr><p>baseline模型预测起始标记和终点标记，他们各自独立生成，在文章里构成一个跨度。采用<a href="http://arxiv.org/abs/1608.07905" target="_blank" rel="noopener">Machine comprehension using match-lstm and answer pointer</a>提出的应答指针，把起始标记反馈给GRU，以指向对应的结束标记。</p><h2 id="适配器和PALs"><a href="#适配器和PALs" class="headerlink" title="适配器和PALs"></a>适配器和PALs</h2><hr><p>适配器在对给定任务微调好了的预训练网络的transformer层添加了新的bottleneck模型。类似的，在transformer层添加带有不同transformer层共享权重的计划注意力层（Projected Attention Layer）。</p><h2 id="从CoQA转化学习"><a href="#从CoQA转化学习" class="headerlink" title="从CoQA转化学习"></a>从CoQA转化学习</h2><hr><p>数据集选用CoQA</p><h1 id="方式"><a href="#方式" class="headerlink" title="方式"></a>方式</h1><h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><hr><p>使用bert-base作为baseline。模型使用两个向量$S，E\in ,\mathbb{R}^H$，其中$H$是每一个输出向量的size。$T_i$是BERT第i个token的最终隐藏向量，每一个$i\in {start, end}$的词的概率计算方式如下：<br>$$P_i=\frac{e^{k.T_j}}{\sum_j{e^{k.T_j}}} | K \in {S,E}$$</p><p>设置了一个阈值来决定问题是否可回答。这是把BERT应用到AQuAD最简单的方式。本文使用了Huggingface Pytorch实现的修改版本，增加了训练/验证追踪和其他改变。</p><h2 id="架构探索"><a href="#架构探索" class="headerlink" title="架构探索"></a>架构探索</h2><hr><p>模型的两个主要目标：</p><ul><li>提升在SQuAD数据集上的整体性能</li><li>保持最小的存储开销</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文研读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 11 - Question Answering</title>
      <link href="/2021/12/12/cs224n-lecture-11-question-answering/"/>
      <url>/2021/12/12/cs224n-lecture-11-question-answering/</url>
      
        <content type="html"><![CDATA[<p>QA系统的模型有两种，一种是基于LSTM的attention模型，一种是在bert模型基础上微调。</p><h1 id="BiDAFF：the-Bidirectional-Attention-Flow-model"><a href="#BiDAFF：the-Bidirectional-Attention-Flow-model" class="headerlink" title="BiDAFF：the Bidirectional Attention Flow model"></a>BiDAFF：the Bidirectional Attention Flow model</h1><hr><p>整体结构从下往上：</p><ul><li>字符嵌入层</li><li>词嵌入层</li><li>短语嵌入层</li><li>Attention Flow层</li><li>modeling层</li><li>输出层<br><img src="1.png" alt></li></ul><h2 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h2><hr><p>编码部分包括了字符嵌入、词嵌入、短语嵌入层。<br>对文本和问题，分别作编码工作。字符嵌入通过CNN和max pooling实现，词嵌入使用GloVe实现串联，然后各自使用2个Bi-LSTM产生文本和问题的上下文关联嵌入向量。<br><img src="2.png" alt></p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><hr><p>Attention Flow层用于获取文本和问题之间的关联，在BiDFA论文中有两种表示方式：</p><ul><li>从文本到问题的注意力：对文本的每个词，匹配问题中最相关的词</li><li>从问题到文本的注意力：对于问题中的一些词，在文本中选择和（每一个）他们最相关的词</li></ul><p>实现方式如下：<br>第一步，$c_i$是文本中每一对上下文向量，$q_j$是问题中的每一对上下文向量，把$c_i$、$q_j$、$c_i$和$q_j$的逐个元素相乘的乘积得到的乘积向量（element-wise multiplication）链接起来，和之前学习得到的$w_{sim}^{T}，w_sim\in \mathbb{R}^{6H}$向量做内积，得到相似度分数$S_{i,j}$,一个数或者标量。<br>第二步，计算从文本到问题的注意力。$S_{i,j}$矩阵的每一行对应一个文本单词，对每一行做softmax，得到$\alpha_{i,j}$，然后和每个问题向量$q_j$相乘求和，得到$a_i$。<br>第三步，计算从问题到文本的注意力。对$S_{i,j}$每一行取最大值，表示和问题最相关的单词，然后做softmax得到$\beta_i$，和文本向量$c_i$相乘累加得到$b_i$。<br>最后将$c_i$,$a_i$,$c_i\, a_i$的每个元素内积的和，$c_i\, b_i$的每个元素内积的和链接起来，得到最终的输出，代表文本里的某个词。<br><img src="3.png" alt></p><h2 id="模型和输出层"><a href="#模型和输出层" class="headerlink" title="模型和输出层"></a>模型和输出层</h2><hr><p>模型层把Attention层的输出$g_i$传递给两个Bi-LSTM层。</p><ul><li>注意力层对问题和文本的词之间的相互影响建模</li><li>模型层对文本内的词的相互影响建模</li></ul><p>$$m_i = BiLSTM(g_i)\in \mathbb(R)^{2H}$$</p><p>输出层实质是两个分类器，分别预测答案在文本中过的起点和终点。<br>连接$g_i$和$m_i$，和$w_{start}^{T}$求内积，可以得到文本中每个位置的分数，然后softmax得到答案起点概率。类似，另一个分类器先对$m_i$做BiLSTM，然后和$g_i$连接起来，和$w_{end}^{T}$求内积，得到答案终点的概率。计算公式如下：</p><p>$$p_{start} = softmax(w_{start}^{T}[g_i;m_i])$$</p><p>$$m_i^\prime = BiLSTM(m_i)\in \mathbb{R}^{2H}\,\\p_{end} = softmax(w_{end}^{T}[g_i;m_i^\prime ])\,\,\,\,\,\,w_{start},w_{end}\in \mathbb{R}^{10H}$$</p><p>$$\iota(loss)= -log\,p_{start}(s^{*}) - log\,p_{end}(e^{e})$$</p><p><img src="4.png" alt></p><h1 id="BERT模型在阅读理解的应用"><a href="#BERT模型在阅读理解的应用" class="headerlink" title="BERT模型在阅读理解的应用"></a>BERT模型在阅读理解的应用</h1><hr><p>BERT模型是深度双向Transformer在大文本上的编码预训练，主要训练掩饰语言模型masked language model(MLM)和对下个句子的预测next sentence prediction(NSP)。BERT-base有12层1.1亿个参数，BERT-large有24层3.3亿个参数。<br>BERT模型把问题作为片段A，文本片段B，拼接起来作为输入，进行句子嵌入，答案是在片段B里对两个端点的预测，用两个softmax得到，损失函数和MiDAF一样。<br><img src="5.png" alt></p><h2 id="BERT和BiDAF的比较"><a href="#BERT和BiDAF的比较" class="headerlink" title="BERT和BiDAF的比较"></a>BERT和BiDAF的比较</h2><hr><p>两者的区别在于：</p><ul><li>BERT模型具有比BiDAF更多的参数。</li><li>BERT建立于Transformers——没有循环结构并且易于并行，BiDAF建立于数个BiLSTM。</li><li>BERT是预训练模型，BIDAF建立于GloVe——所有参数都需要从有监督数据集学习。</li></ul><p>两者的相似点在于：</p><ul><li>模型都旨在建立问题和文本之间的相互影响关系</li><li>BERT模型使用了问题和文本的连接的自我注意力。<br>attention(Paragraph,P) + attention(P,Question) + attention(Q,P) + attention(Q,Q)</li><li>研究表明，对BIDAF添加文本注意力attention(P,P)构建自我注意力层，性能会得到提升</li></ul><h2 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h2><hr><p>SpanBERT模型修改了BERT的mask。BERT是随机选取15%的词mask，SpanBERT选择连续词块，使用两个词块的端点去预测所有掩饰掉的词——把所有的块内信息集中在端点上。<br>$$y_i = f(x_{s-1}),x_{e+1},p_{i-s+1}$$</p><p><img src="6.png" alt></p><p>但是机器阅读理解的依然没有解决。现在的模型往往只能对一类语料库测试表现良好，泛化能力欠佳。哪怕是BERT-large也不能处理一些简单的问题。</p><h1 id="开放领域问答"><a href="#开放领域问答" class="headerlink" title="开放领域问答"></a>开放领域问答</h1><hr><p>不同于机器阅读理解，开放领域问答假设拥有大量文档（如Wikipedia），但是不知道答案在什么位置，任务的目标是返回任意开放问题的答案。对之建立retrieve-reader框架。<br>在DRQA里，</p><ul><li>输入：大量文档，问题</li><li>输出：代表答案的字符串</li><li>retriever：检索得到和问题最相关的K个文档，K是预先定义的小数值，例如100。是一个标准的TF-IDF信息检索模型。</li><li>reder：在K个文档里找出答案A。是一个学习得到的自然阅读理解模型，在SQuAD和其他远距离监督QA数据集上训练得到。</li></ul><p>可以对retriver进行训练。对retriver和reader联合训练，每一个文本可以用BERT编码为向量，问题分布和文章分布的内积可以作为信息检索分数。但是这在有大量文章的情况下不容易建模。<br><img src="7.png" alt></p><p>稠密文章检索Dense passage retrieval(DPR)，使用问题-答案对，对检索器进行训练，可以获得比常规信息检索模型更好的表现。</p><p>近期的工作表明，用生成答案的方式取代提取答案的方式，是有益的。</p><p>大模型，例如T5，不使用明确的检索器，可以在开放领域问答取得很好的效果，<br><img src="8.png" alt></p><p>也有不使用reader，把所有短语用密集向量编码，在推理时候脱离BERT模型，只做近邻搜索的方式。<br><img src="9.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> cs224n </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention Is All Your Need</title>
      <link href="/2021/12/11/attention-is-all-your-need/"/>
      <url>/2021/12/11/attention-is-all-your-need/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/abs/1706.03762v5" target="_blank" rel="noopener">Attention Is All Your Need</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><hr><p>在2017年，LSTM在句子模型和翻译问题中是最先进的技术，常规RNN将位置与计算时间中的步骤对齐，它们生成一个隐藏状态序列ht，作为前一个隐藏状态ht-1和位置t的输入的函数。这种线性顺序的计算不利于并行化。Attention机制允许建立依赖关系，不考虑计算过程中输入和输出序列的距离问题。但绝大部分时候和RNN结合在一起使用。Transformer模型是首个完全依赖attention机制实现输入和输出序列全局依赖的模型，具有良好并行性。</p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><hr><p>减少顺序计算的目标，是拓展神经GPU、位图和ConvS2S的基础。所有这些都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。在这些模型里,两个来自任意输入或者输出位置的相关信号需要的操作数，随着距离间隔增长，在ConvS2S和ByteNet是线性增长。这使得学习两个遥远位置的依赖变得困难。在Transformer中操作数被减少到常数级，尽管以通过平均化attention权重位置减少有效分辨率（resolution）为代价，本文将之描述为多头注意力。</p><p>自我注意力（Self-attention），有时候也被称为内部注意力，是一种将单个序列的不同位置联系起来以计算序列表示的机制。自我注意力在很多领域被广泛使用，例如阅读理解、摘要总结、文本蕴涵和学习任务无关的句子表示。</p><p>端到端记忆网络，基于重复注意机制而不是序列排列的重复，已被证明在简单语言问题回答和语言建模任务中表现良好。</p><p>Transformer是首个基于自我注意力计算输入输出的分布表达，而不是序列对齐RNN或者卷积的转换模型。</p><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><hr><p>Encdoer把输入序列$(x_1,\ldots,x_n)$和连续分布$z=(z_1,\ldots,z_n)$连接起来。给定$z$，decoder会生成符号序列$(y_1,\ldots, y_m)$，每次一个元素。在每一个时刻，模型都是自回归的，在生成下一个符号时，把之前生成的符号作为额外信息附加到输出上。<br>Transformer模型整体架构使用堆叠的自注意力和点方向，把encoder和decoder层完全连接起来。<br><img src="model%20architecture.png" alt="Transformer整体架构"></p><h2 id="Encoder和Decoder的堆栈"><a href="#Encoder和Decoder的堆栈" class="headerlink" title="Encoder和Decoder的堆栈"></a>Encoder和Decoder的堆栈</h2><hr><p>Encoder：encoder由6个独立层构成，每一层有两个自称。第一层是多头自我注意力机制，第二层是简单的位置智能的全连接前馈网络。在每个子层使用残差连接，做层标准化处理。就是说，每一子层的输出是$LayerNorm(x+Sublayer(x))$，$SubLayer(x)$是由子层实现的函数。所有的子层，和embadding层一样，产生的输出维度是512。</p><p>Decoder：decoder也是6个独立层构成。在两个子层的基础上，增加了一个子层，表示对encoder输出的多头注意力。和encoder层一样，每个子层用残差连接，并且做标准化处理。同时对decoder层中的自我注意力子层做了修改，以防止在对当前位置计算时，把后续位置的信息也纳入计算。通过masking，并且把输出embedding偏移一个位置，确保了对位置i的预测只能依赖于位置小于i的已知输出。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><hr><p>Attention函数可以被描述为把query和一系列key-value的映射作为output，query、keys、values和output都是向量。Output是values的权重和，分配给每个value的权重由query的兼容函数与相应的key计算。</p><h3 id="Scaled-Dot-product-Attention"><a href="#Scaled-Dot-product-Attention" class="headerlink" title="Scaled Dot-product Attention"></a>Scaled Dot-product Attention</h3><hr><p>把特定的attention称为“Scaled Dot-Product Attention”。输入由$d_k$维的query和key、$d_v$维的value组成。计算query和所有的key的内积，每一项除以$\sqrt{d_k}$，通过softmax函数获得value的权值。</p><p>把一系列query的Attention函数的输出用矩阵Q表示，key和value用矩阵K和V表示，矩阵的输出可以表示为：<br>$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})\tag{1}$$</p><p>使用最广泛的attention函数是additive attention和dot-product attention。除了计算缩放因子$(\frac{1}{\sqrt{d_k}})，$Dot-product attention在整个算法中是一致的。Additive attention通过前馈网络计算单个隐藏层的compatibility函数。这两个函数理论上复杂度相近，但实际上dot-product attention更快并且有更好的空间效率，因此可以用高度优化的矩阵乘法代码实现。</p><p>对于较小的$d_k$这两种方式表现相近，在不实用缩放因子$d_k$时additive attention表现得更好。作者认为对于较大值的$d_k$，内积变得非常巨大，使对应softmax函数区域的梯度变得非常小。为了抵消这方面的影响，把内积缩小为$\frac{1}{\sqrt{d_k}}$。</p><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><hr><p>相比较使用$d_{model}$-demensional的query、key、value的单一attention函数，用不同的学习得到的线性投影，把key、value、query分别线性映射h次到$d_k$，$d_k$，和$d_v$维上更加有益。在每一次显现投影，并行调用attention函数，生成$d_v$-dimensional的输出值。把它们连接起来并再次映射，最后得到最终的value。多注意力头允许模型把来自不同分布子空间不同位置的信息结合起来。通过单个注意力头，平均化inhibit。<br>$$MultiHead(Q,K,V) = Concat(head_1,\ldots,head_h)W^O\\where\,head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$<br>映射矩阵$W_i^Q\in \mathbb{R}^{d_{model}\times d_k}$，$W_i^K\in \mathbb{R}^{d_{model}\times d_k}$，$W_i^V\in \mathbb{R}^{d_{model}\times d_v}$，$W^O\in \mathbb{R}^{hd_{v}\times d_model}$。<br>设置h=8层并行的attention层或者注意力头，$d_k=d_v=\frac{d_model}{h}=64$。优于注意力头的降维效果，整个计算的开销和不降维的单注意力头attention相似。<br><img src="Figure2.png" alt></p><h3 id="模型中attention的应用"><a href="#模型中attention的应用" class="headerlink" title="模型中attention的应用"></a>模型中attention的应用</h3><hr><p>Transformer在三方面使用多注意力头：</p><ul><li>在“encoder-decoder”层，query来自前一decoder层，存储的key和value来自前一encoder层。使得decoder的每一个位置都和整个输入序列关联起来。这个实现方式模仿了Seq2Seq模型中经典的encoder-decoder attention机制。</li><li>encoder包含了自我注意力层。在自我注意力层汇总所有的key、value、query都来自前一encoder层的output。Encoder的每一个位置都和前一encoder层关联起来。</li><li>decoder的自我注意力层允许decoder的每一个位置，和decoder中的其他所有位置，包括自身位置关联起来。需要防止信息的向左流动，以保持自回归特性。在scaled dot-product attention中，通过对softmax的非法链接的所有value做mask——把value设置成-∞实现。</li></ul><h2 id="position-wise-前馈网络"><a href="#position-wise-前馈网络" class="headerlink" title="position-wise 前馈网络"></a>position-wise 前馈网络</h2><hr><p>每一个attention子层，包含一个全连接前馈网络，独立且相同地应用在每个位置上。前馈网络使用RelU激活并包含两个线性转换。<br>$$FFN(x)=max(0,xW_1+b_1)W_2+b_2\tag{2}$$<br>对于不同位置，线性转换的方式是相同的，但不同层使用不同的参数值。另一种方式描述是两个卷积，内核大小是1。输入和输入的维度$d_{model}=512$，inner-layer的维度$d_{ff}=2048$</p><h2 id="Embedding和Softmax"><a href="#Embedding和Softmax" class="headerlink" title="Embedding和Softmax"></a>Embedding和Softmax</h2><hr><p>和其他句子转换模型相似，Transformer使用学习得到的embedding把输入的token和输出的token转变为$d_{model}$维度的向量。同时使用常规的学习得到的线性转换和softmax函数，把decoder的输出转变成后续token可能性的预测。两个embedding层之间和pre-softmax线型转变共用相同的权重矩阵。在embedding层，把权重乘以$\sqrt{d_{model}}$。</p><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><hr><p>Transformer不包含循环和卷积，为了让模型利用句子的顺序，需要添加token在句子里相对或者绝对位置的信息。<br>在encoder和decoder堆栈的末端，对输入的embedding添加了位置编码。位置编码有和embedding相同的维度$d_{model}$，所以它们可以相加。位置编码的设置有两种选择，学习或者固定顺序。</p><p>本模型采用不同频度的sin和cos函数：<br>$$PE_(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\ \,\\PE_(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$</p><p>$pos$表示位置，$i$表示维度。位置编码的每一个维度对应一个正弦信号。选择整个函数是因为假设对于任何给定的$$k$$偏置，$$PE_{pos+k}$$都可以被表示为$PE_{pos}$的线性表示，模型可以比较容易地学习到。正弦函数可以让模型推断出比在训练中遇到的序列长度更长的句子。</p><h1 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h1><hr><p>把Transformer和RNN、CNN作比较，论证为什么选择Transformer。RNN和CNN处理 把可变长度的句子符号分布$(x_1,\ldots,x_n)$映射到另一个登场的句子$(z_1,\ldots,z_n)$，和Transformer在一个隐藏层里decoder或者encoder对句子做的操作类似。<br>一共有三点，一是每一层的整体计算复杂度，另一点是可并行计算的单元数量——用所需的最小顺序操作数量度量，第三点是网络中长距离依赖的路径长度——网络中不同的层任意两个输入输出的最大路径长度。</p><p><img src="table1.png" alt></p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><hr><p>四个超参，使用Adam优化器，设置$\beta_1=0.9,\beta_2=0.98,\epsilon=10^{-9}$,根据公式在训练中改变学习率：<br>$$lrate=d_{model}^{-0.5}\cdot min(step_num^{-0.5}, step_num\cdot warmup_steps^{-1.5})\tag{3}$$</p><p>使用了两种类型的正则化方式。</p><ul><li>离散Dropout：在每一个子层的output被加入子层和标准化前，对output做dropout。并且对embedding的和，与encoder和decoder的堆栈里位置编码做dropout。设置$P_{drop}=0.1$。</li><li>标签平滑化：设置平滑值$\epsilon_{ls}=0.1$,虽然会增加困惑度，但提升了准确率和BLEU分数。</li></ul><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><hr><p>Transformer模型是是首个完全依托于attention的句子转换模型，用多头自我注意力代替了广泛用于encoder-decoder结构中的循环层。在机器翻译中取得SOTA。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><blockquote><p>Attention Is All Your Need<br>Layer normalization<br>Neural machine translation by jointly learning to align and translate<br>Long short-term memory-networks for machine reading</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> cs224n </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文研读 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N attention学习笔记</title>
      <link href="/2021/12/08/cs224n-attention-xue-xi-bi-ji/"/>
      <url>/2021/12/08/cs224n-attention-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p><img src="1.png" alt><br>Attention</p><p>• 常规Seq2Seq的RNN网络，存在imformation bottleneck问题.如图所示，所有源语句的信息都存储在encoder的一个隐藏状态里，所有的生成信息都基于此形成.attention的设计可以解决这问题.<br>• 核心思想是，在decoder的每一时序，通过decoder和encoder的direct link，使注意力集中在一部分词句上，从而判断接下来是什么词.</p><p><img src="2.png" alt><br>encoder照常，每一个时序的decoder，都会调用encoder每一个位置的隐藏状态信息，作内积，得到attention scores.为每一个隐藏状态打分后，通过softmax得到attention分布.从分布可以看出当前decoder应该focus on在哪个隐藏状态上.</p><p><img src="3.png" alt><br>把attention分布里各项相加，得到attention输出，表示encoder的隐藏状态，包含的信息主要是受到高注意力的那些隐藏状态.<br>将输出和decoder的起始状态链接，得到$\hat y$，预测第一个词的输出，如图所示，是he.<br>之后处理后面位置的decoder，类似，与每个位置的encoder元素的隐藏状态作内积作为打分，重复之前步骤</p><p><img src="4.png" alt></p><p>最后得到完整的输出序列</p><p><img src="5.png" alt></p><p>公式如图.同时attention的表示有若干种变体，以针对不同情形进行改进</p><p><img src="6.png" alt></p><p>对于attention，更为广泛的解释是:</p><blockquote><p>Given a set of vector values， and a vector query， attention is a technique to compute a weighted sum of the values， dependent on the query</p></blockquote><p>给定一个query向量和value向量，attention是一种依据query，计算value权重和的技术.<br>query决定了要在哪些value上pay attention，权重和是这些被挑选出来的value包含的<br>选择性求和.<br>attention是一种获得任意表示集(value)的固定大小表示的方法，它依赖于某些其他表示(query).<br>attention最初被用于机器翻译，后来被拓展到多个方向和领域，并促成transformer和bert的发展.</p><p>对于常规的RNN，是从左往右铺开的，对线性布局启发式编码，相近的词会互相影响含义.但是这样的神经网络需要跨越O(n)步长使得距离为n的单词互相作用.这意味着因为梯度问题长距离依赖很难被处理，并且线性顺序嵌入的单词，不利于句子语义的表示.这样的结构缺少并行性.</p><p><img src="7.png" alt></p><p>先不考虑循环，引入word windows模型</p><p><img src="8.png" alt><br>图片的数字表示可以计算状态之前的最小步数.不可并行操作的数量，不会影响句子的长度.</p><p><img src="9.png" alt><br>堆叠单词窗口层允许更多单词之间的交互.<br>最大交互距离=序列长度/窗口大小，但如果序列太长，就会忽略长距离的上下文.</p><p>回到attention上，attention可以把每个词的表示视作一个query，合并来自一系列value表示的信息.前面说的attention是从encoder到decoder，在这attention用在单个句子里.</p><p><img src="10.png" alt><br>每一层都可以和前一层的所有单词交互</p><p>引入self-attention概念<br><img src="11.png" alt><br>每个词都有qkv，从第二层开始，每一层的qkv取值为前一层的输出x.依次计算key-query的关联度，从关联度计算attention权重，把权重和作为value.</p><p><img src="12.png" alt><br>self-attention是在set上的操作，是无序的，输出的顺序是未知的.</p><p><img src="13.png" alt><br>对于第一个问题，把位置信息编码进kqv，把句子序号作为一个向量，和每个qkv相加</p><p><img src="14.png" alt></p><p>对于p的表示有两种思路<br><img src="15.png" alt><br>一是用正弦曲线向量表示位置，优点在于绝对位置不那么重要，正弦曲线有周期性，可以推断更长的序列</p><p>二是从头开始，用绝对位置表示，每个$p_i$表示一列，$p_i$是一个可学习的参数.优点是每个位置数据都可以被学习.</p><p>对于第二个问题，在self-attention中加入线性相关性<br>对没一个输出向量，增加一个前馈神经网络.<br><img src="16.png" alt></p><p>对于第三个问题，把一部分将来会出现的单词遮掩，在每一时序，改变keys和values，使之仅包含过去出现过的词.考虑并行性，设置未出现的词的attention分数为-∞.</p><p><img src="18.png" alt></p><p><img src="19.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> cs224n </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/12/08/hello-world/"/>
      <url>/2021/12/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
