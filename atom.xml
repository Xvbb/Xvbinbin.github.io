<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>徐斌斌的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xvbinbin.xyz/"/>
  <updated>2021-12-17T12:44:21.835Z</updated>
  <id>https://xvbinbin.xyz/</id>
  
  <author>
    <name>徐斌斌</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding论文研读</title>
    <link href="https://xvbinbin.xyz/2021/12/17/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-lun-wen-yan-du/"/>
    <id>https://xvbinbin.xyz/2021/12/17/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-lun-wen-yan-du/</id>
    <published>2021-12-17T06:38:02.000Z</published>
    <updated>2021-12-17T12:44:21.835Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;论文地址:&lt;a href=&quot;https://arxiv.org/pdf/1810.04805&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BERT Pre-training of Deep Bidirectional Transformers for
        
      
    
    </summary>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="nlp" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/nlp/"/>
    
    
      <category term="nlp" scheme="https://xvbinbin.xyz/tags/nlp/"/>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文研读" scheme="https://xvbinbin.xyz/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
      <category term="语言模型" scheme="https://xvbinbin.xyz/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>BERT-A:Fine-tuning BERT with Adapters and Data Augmentation学习笔记</title>
    <link href="https://xvbinbin.xyz/2021/12/17/bert-a-fine-tuning-bert-with-adapters-and-data-augmentation-xue-xi-bi-ji/"/>
    <id>https://xvbinbin.xyz/2021/12/17/bert-a-fine-tuning-bert-with-adapters-and-data-augmentation-xue-xi-bi-ji/</id>
    <published>2021-12-17T04:42:34.000Z</published>
    <updated>2021-12-17T09:17:34.012Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;论文地址:&lt;a href=&quot;https://arxiv.org/pdf/1810.04805&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BERT-A:Fine-tuning BERT with Adapters and Data
        
      
    
    </summary>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="nlp" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/nlp/"/>
    
    
      <category term="nlp" scheme="https://xvbinbin.xyz/tags/nlp/"/>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文研读" scheme="https://xvbinbin.xyz/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>随笔</title>
    <link href="https://xvbinbin.xyz/2021/12/14/sui-bi/"/>
    <id>https://xvbinbin.xyz/2021/12/14/sui-bi/</id>
    <published>2021-12-14T12:59:08.000Z</published>
    <updated>2021-12-14T12:59:08.887Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CS224N Lecture 12 Natural Language Generation</title>
    <link href="https://xvbinbin.xyz/2021/12/14/cs224n-lecture-12-natural-language-generation/"/>
    <id>https://xvbinbin.xyz/2021/12/14/cs224n-lecture-12-natural-language-generation/</id>
    <published>2021-12-14T06:42:39.000Z</published>
    <updated>2021-12-14T12:58:31.117Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="nlp" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/nlp/"/>
    
    
      <category term="nlp" scheme="https://xvbinbin.xyz/tags/nlp/"/>
    
      <category term="cs224n" scheme="https://xvbinbin.xyz/tags/cs224n/"/>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="https://xvbinbin.xyz/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>CS224N Lecture 11 - Question Answering</title>
    <link href="https://xvbinbin.xyz/2021/12/12/cs224n-lecture-11-question-answering/"/>
    <id>https://xvbinbin.xyz/2021/12/12/cs224n-lecture-11-question-answering/</id>
    <published>2021-12-12T14:55:39.000Z</published>
    <updated>2021-12-14T06:42:46.696Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;QA系统的模型有两种，一种是基于LSTM的attention模型，一种是在bert模型基础上微调。&lt;/p&gt;
&lt;h1 id=&quot;BiDAFF：the-Bidirectional-Attention-Flow-model&quot;&gt;&lt;a
        
      
    
    </summary>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="nlp" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/nlp/"/>
    
    
      <category term="nlp" scheme="https://xvbinbin.xyz/tags/nlp/"/>
    
      <category term="cs224n" scheme="https://xvbinbin.xyz/tags/cs224n/"/>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="https://xvbinbin.xyz/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Attention Is All Your Need</title>
    <link href="https://xvbinbin.xyz/2021/12/11/attention-is-all-your-need/"/>
    <id>https://xvbinbin.xyz/2021/12/11/attention-is-all-your-need/</id>
    <published>2021-12-10T17:40:36.000Z</published>
    <updated>2021-12-12T14:53:31.426Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;论文地址:&lt;a href=&quot;https://arxiv.org/abs/1706.03762v5&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention Is All Your Need&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;介绍&quot;&gt;&lt;a
        
      
    
    </summary>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="nlp" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/nlp/"/>
    
    
      <category term="nlp" scheme="https://xvbinbin.xyz/tags/nlp/"/>
    
      <category term="cs224n" scheme="https://xvbinbin.xyz/tags/cs224n/"/>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文研读" scheme="https://xvbinbin.xyz/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
      <category term="Transformer" scheme="https://xvbinbin.xyz/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>CS224N attention学习笔记</title>
    <link href="https://xvbinbin.xyz/2021/12/08/cs224n-attention-xue-xi-bi-ji/"/>
    <id>https://xvbinbin.xyz/2021/12/08/cs224n-attention-xue-xi-bi-ji/</id>
    <published>2021-12-08T08:33:54.000Z</published>
    <updated>2021-12-10T17:42:07.664Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;&lt;img src=&quot;1.png&quot; alt&gt;&lt;br&gt;Attention&lt;/p&gt;
&lt;p&gt;• 常规Seq2Seq的RNN网络，存在imformation
        
      
    
    </summary>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="nlp" scheme="https://xvbinbin.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/nlp/"/>
    
    
      <category term="nlp" scheme="https://xvbinbin.xyz/tags/nlp/"/>
    
      <category term="cs224n" scheme="https://xvbinbin.xyz/tags/cs224n/"/>
    
      <category term="深度学习" scheme="https://xvbinbin.xyz/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="https://xvbinbin.xyz/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://xvbinbin.xyz/2021/12/08/hello-world/"/>
    <id>https://xvbinbin.xyz/2021/12/08/hello-world/</id>
    <published>2021-12-08T08:07:49.082Z</published>
    <updated>2021-12-08T08:07:49.082Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a
        
      
    
    </summary>
    
    
  </entry>
  
</feed>
